{"prompt_type": "Factual Question", "system_prompt": "You are an expert in generating factual questions.", "user_prompt": "Factual questions are those with a single, precise answer. Based on the config, please generate factual questions. These questions should primarily come from the facts of the crime section in the config. The steps and requirements are as follows:\n\n1. First, filter key-value pairs from the outline that can form factual questions;\n2. Directly generate or summarize the key-value pairs of factual questions: if the value type belongs to names, numbers, locations, entities, indicators, nouns, or noun phrases, keep them unchanged; otherwise, summarize the key-value pair again based on the value. If the summarized value is still a sentence, discard this key-value pair;\n3. Generate factual questions and corresponding answers based on the key-value pairs. The questions should be clear, with a single standard answer. Ensure that the question types are as diverse as possible and not all questions have the same answer type. The questions should start with \"According to the court judgment of xx court\", and the answers should include the following types: names, numbers, locations, entities, indicators, nouns, or noun phrases.\n\nBelow is a set of sample questions and answers, intended to guide you on how to format the generated factual questions and corresponding answers:\nSample questions:\n[\n    {{\n        \"question type\": \"Factual Question\",\n        \"question\": \"According to the judgment of Apple Court, what crime did Zhang commit?\",\n        \"answer\": \"Intentional injury.\"\n    }},\n    {{\n        \"question type\": \"Factual Question\",\n        \"question\": \"According to the judgment of Apple Court, what was Zhang's trial result?\",\n        \"answer\": \"Three years of imprisonment.\"\n    }}\n]\n\nPlease refer to the structure of the above examples and construct factual questions, question content, and answers based on the provided config. Output in JSON array format.\n\nThe config is as follows: {config}\n"}
{"prompt_type": "Multi-hop Reasoning Question", "system_prompt": "You are an expert in generating multi-hop reasoning questions.", "user_prompt": "Multi-hop reasoning questions require us to start from multiple related events and details in the config, and through logical reasoning, progressively reach the answer to the question. These questions need to cover multiple different steps or logical connection points to ensure coherence and complexity in reasoning. Please follow the steps below to accurately construct multi-hop reasoning questions:\n\n1. Determine the starting point and endpoint: Identify a clear starting point and endpoint, analyzing the logical relationship between them. The endpoint should be chosen from the following fields: maximum amount of crime, number of crimes, total amount of crime, number of pieces of evidence, etc.\n2. Design intermediate steps: Design multiple logical reasoning steps between the starting point and endpoint. Each step should be connected with the previous and next steps, forming a reasoning chain.\n3. Construct the question: Link these steps into a complete question. The question should require the respondent to sequentially go through all the intermediate reasoning steps, integrate the corresponding fragments, and perform logical reasoning based on these fragments to arrive at the final answer. Ensure the question is clearly expressed and logically coherent. The question should start with \"According to the judgment of xx court\".\n4. Generate the answer: Based on the question and outline, generate the answer. Ensure that the reasoning process and steps for the answer are complete and reasonable.\n\nSample question types, starting points, endpoints, intermediate steps, question content, and answers:\n[\n    {{\n        \"question type\": \"Multi-hop Reasoning Question\",\n        \"starting point\": \"Yambo City Apple District People's Court judgment\",\n        \"endpoint\": \"Maximum amount of crime\",\n        \"intermediate steps\": [\"stealing a phone\", \"stealing a handbag\", \"stealing a wallet\", \"stealing a watch\"],\n        \"question\": \"According to the judgment of Yambo City Apple District People's Court, what was the item with the highest value stolen by Luo in all his crimes?\",\n        \"answer\": \"According to the judgment, the LV handbag stolen by Luo on February 20, 2023, worth 5,000 yuan, is the most valuable item among all the crimes.\"\n    }},\n    {{\n        \"question type\": \"Multi-hop Reasoning Question\",\n        \"starting point\": \"Yambo City Apple District People's Court judgment\",\n        \"endpoint\": \"Number of crimes\",\n        \"intermediate steps\": [\"stealing a phone\", \"stealing a handbag\", \"stealing a wallet\", \"stealing a watch\"],\n        \"question\": \"According to the judgment of Yambo City Apple District People's Court, how many theft crimes did Luo commit in total?\",\n        \"answer\": \"According to the judgment, Luo committed a total of four theft crimes: stealing a phone, a handbag, a wallet, and a watch each once.\"\n    }},\n    {{\n        \"question type\": \"Multi-hop Reasoning Question\",\n        \"starting point\": \"Yambo City Apple District People's Court judgment\",\n        \"endpoint\": \"Total amount of crime\",\n        \"intermediate steps\": [\"stealing a phone\", \"stealing a handbag\", \"stealing a wallet\", \"stealing a watch\"],\n        \"question\": \"According to the judgment of Yambo City Apple District People's Court, what is the total amount of the stolen items in all of Luo's crimes?\",\n        \"answer\": \"On February 5, 2023, Luo stole an Apple phone worth 3,000 yuan, on February 20, 2023, he stole an LV handbag worth 5,000 yuan, on March 3, 2023, he stole a wallet containing 1,200 yuan in cash and several bank cards, and on March 10, 2023, he stole a Gucci watch worth 8,000 yuan. The total amount is 3,000 + 5,000 + 1,200 + 8,000 = 17,200 yuan.\"\n    }}\n]\n\nPlease refer to the structure and content of the above examples, and construct multi-hop reasoning questions, starting points, endpoints, intermediate steps, question content, and answers based on the provided config. You can create questions similar to the examples. Ensure the questions and answers are clear and specific, and output them in JSON array format.\n\nThe outline is as follows: {config}\n"}
{"prompt_type": "Summarization Question", "system_prompt": "You are an expert in generating summary questions.", "user_prompt": "Summary questions require us to comprehensively summarize the key information in the config regarding the court judgment. These questions should cover multiple aspects of information, requiring the respondent to comprehensively analyze and extract information from the config. Please follow the steps below to accurately construct summary questions:\n\n1. Determine the summary content: Analyze the outline to determine the content and scope of information that needs to be summarized, such as facts of the crime, evidence collection, etc.\n2. Extract key information: Extract key events related to the summary content from the outline.\n3. Construct the question: Based on the extracted key information, construct a comprehensive and clear summary question. The question should require the respondent to comprehensively analyze and summarize multiple aspects of information. The question should start with \"According to the judgment of xxx court\".\n4. Generate the answer: Based on the question and outline, generate the answer. Ensure that the answer covers all key points, is logically clear, and accurately expressed.\n\nSample question types, summary content, question content, and answers:\n[\n    {{\n        \"question type\": \"Summarization Question\",\n        \"summary content\": \"Facts of the crime\",\n        \"question\": \"According to the judgment of Apple Court, summarize the facts of Zhang's crimes.\",\n        \"answer\": \"Zhang stole a phone worth 2,000 yuan on November 1, 2020, a wallet worth 40,000 yuan on December 5, 2018, and a watch worth 3,000 yuan in January 2023.\"\n    }},\n    {{\n        \"question type\": \"Summarization Question\",\n        \"summary content\": \"Evidence of the crime\",\n        \"question\": \"According to the judgment of Apple Court, summarize the evidence of Zhang's crimes.\",\n        \"answer\": \"Surveillance footage, fingerprints, on-site investigation, witness testimonies.\"\n    }}\n]\n\nPlease refer to the structure of the above examples and construct summary questions, summary content, question content, and answers based on the provided outline. Output in JSON array format.\n\nThe outline is as follows: {config}\n"}
{"prompt_type": "Multi-document Information Integration Question", "system_prompt": "You are an expert in generating question answers.", "user_prompt": "Multi-document information integration questions require gathering multiple information fragments from multiple configs to arrive at an answer. The goal of this task is to identify and utilize the common points between the two configs to generate multi-document information integration questions based on these common points. Please follow the steps below:\n\n1. Identify common points: Analyze the provided configs and identify the common points between them, giving one keyword for each common point.\n2. Construct the question: Design a question based on each common point without including multiple sub-questions. Ensure that answering the question requires synthesizing information from both configs. The expression should be clear and precise, with no ambiguity. Avoid questions that contain two or more sub-questions, such as \"How much funding was raised through large-scale financing activities in the 2020 financial report of Aerospace Corporation, and what impact did it have on the company?\" This contains two sub-questions: \"How much funding was raised?\" and \"What impact did it have on the company?\" which is not allowed. The question should start with \"According to the judgment of xx court and xx court\".\n3. Generate the answer: Based on the common points and the question, generate the answer. Ensure that the answer requires gathering and integrating information fragments from both configs. The answer should be logically clear, fluent, and coherent.\n\nSample question types, common points, questions, and answers:\n[\n    {{\n        \"question type\": \"Multi-document Information Integration Question\",\n        \"common point\": \"Defendant's occupation\",\n        \"question\": \"According to the judgment of Apple Court and Pear Court, what are the occupations of the defendants Zhang and Li?\",\n        \"answer\": \"According to the judgment of Apple Court, the defendant Zhang is a doctor; according to the judgment of Pear Court, the defendant Li is a lawyer.\"\n    }},\n    {{\n        \"question type\": \"Multi-document Information Integration Question\",\n        \"common point\": \"Type of crime\",\n        \"question\": \"According to the judgment of Apple Court and Pear Court, what crimes did the defendants Zhang and Li commit respectively?\",\n        \"answer\": \"According to the judgment of Apple Court, the defendant Zhang committed intentional injury; according to the judgment of Pear Court, the defendant Li committed theft.\"\n    }}\n]\n \nExamples of questions that are not allowed:\n[\n    {{\n        \"question type\": \"Multi-document Information Integration Question\",\n        \"common point\": \"Risk management measures\",\n        \"question\": \"In which year did Great Education Technology Co., Ltd. and Joy Entertainment Group respectively implement updated risk management measures? What aspects did these measures mainly involve?\",\n        \"Explanation\": \"The question contains two sub-questions: 'In which year did Great Education Technology Co., Ltd. and Joy Entertainment Group respectively implement updated risk management measures?' and 'What aspects did these measures mainly involve?' This is not allowed. Each question can only have one sub-question.\"\n    }}\n]\n\nPlease refer to the structure of the above examples and construct multi-document information integration questions, common points, question content, and answers based on the provided configs. Output in JSON array format.\nConfig1 is as follows: {config_1}\nConfig2 is as follows: {config_2}\n"}
{"prompt_type": "Multi-document Comparison Question", "system_prompt": "You are an expert in generating question answers.", "user_prompt": "Multi-document comparison questions require comparing relevant data and the sequence of events from the two configs provided to deeply understand the similarities and differences between the two cases. Please follow these steps to construct multi-document comparison questions:\n\n1. Identify common points: Analyze the outlines and identify data points that can be compared, giving one keyword for each common point.\n2. Construct the question: When designing the question, ensure that the answer requires synthesizing multiple parts of the configs. The question should be clear, precise, and unambiguous. The indicators or times in the question must be comparable and not a simple combination of two separate questions. The question should start with \"According to the judgment of xx court and xx court\".\n3. Generate the answer: Based on the common points and the question, generate the answer. Ensure that the answer requires gathering and integrating information fragments from the configs. The answer should be logically clear and fluent.\n\nSample question types, common points, questions, and answers:\n[\n    {{\n        \"question type\": \"Multi-document Comparison Question\",\n        \"common point\": \"Sentencing time\",\n        \"question\": \"According to the judgment of Apple Court and Pear Court, whose sentencing time is longer, Zhang or Li?\",\n        \"answer\": \"Zhang was sentenced to three years of imprisonment, while Li was sentenced to life imprisonment. Li's sentencing time is longer.\"\n    }},\n    {{\n        \"question type\": \"Multi-document Comparison Question\",\n        \"common point\": \"Total crime amount\",\n        \"question\": \"According to the judgment of Apple Court and Pear Court, whose total crime amount is greater, Zhang or Li?\",\n        \"answer\": \"Zhang's total crime amount is greater.\"\n    }},\n    {{\n        \"question type\": \"Multi-document Time Sequence Question\",\n        \"common point\": \"Judgment time\",\n        \"question\": \"According to the judgment of Apple Court and Pear Court, whose judgment time is earlier, Zhang or Li?\",\n        \"answer\": \"Li's judgment time is earlier.\"\n    }}\n]\n \nPlease refer to the structure of the above examples and construct multi-document comparison questions, common points, question content, and answers based on the provided configs. Output in JSON array format.\nConfig1 is as follows: {config_1}\nConfig2 is as follows: {config_2}\n"}
{"prompt_type": "single document reference", "system_prompt": "You are an expert in matching questions with corresponding reference answers.", "user_prompt": "Please find the reference for the provided questions and answers in the article, and optimize the answers based on the references. Follow these steps:\n\n1. For each provided question and answer, find the source fragments in the article, also known as references. If multiple sentences serve as references, organize them as an array. You need to find all the references in the article, not just one or two sentences, recalling all relevant fragments. References can be based on answer key points but should not be limited to the answer. Try to find as many references as possible in the article. Avoid summary sentences as references; if a reference is a summary sentence, ensure that all the information points mentioned in the answer are described in individual sentences in the reference array. References must be verbatim from the article without any deletion or rewriting. If no corresponding reference is found, output an empty string and do not include unrelated sentences.\n2. Optimize the existing answer based on the references. If the reference contains content not in the answer, supplement the answer. If the answer contains content not found in the reference, check the article for any corresponding references. If there are missing references, add them to the reference array and keep the answer unchanged. If no corresponding reference is found in the article, remove the unrelated content from the answer. Do not stack the answer based on the reference. The answer should be logically clear, fluent, concise, and straightforward. If the current references do not provide the answer to the question, output \"Unable to answer\" in the answer field.\n\nBelow is a set of sample questions, references, and answers to guide you on how to format the output:\n\nSample questions, references, and answers:\n[\n    {{\n        \"question type\": \"Factual Question\",\n        \"question\": \"According to the judgment of Apple Court, what crime did Zhang commit?\",\n        \"ref\": [\n            \"According to the judgment of Apple Court, Zhang committed intentional injury.\"\n        ],\n        \"answer\": \"Intentional injury.\"\n    }},\n    {{\n        \"question type\": \"Multi-hop Reasoning Question\",\n        \"question\": \"According to the judgment of Apple Court, what was the highest value of stolen goods in Zhang's crimes?\",\n        \"ref\": [\n            \"Zhang stole a phone worth 2,000 yuan on November 1, 2020.\",\n            \"Zhang stole a wallet worth 40,000 yuan on December 5, 2018.\",\n            \"Zhang stole a watch worth 3,000 yuan in January 2023.\"\n        ],\n        \"answer\": \"Zhang stole a phone worth 2,000 yuan on November 1, 2020, a wallet worth 40,000 yuan on December 5, 2018, and a watch worth 3,000 yuan in January 2023. The highest value of stolen goods is 40,000 yuan.\"\n    }},\n    {{\n        \"question type\": \"Summarization Question\",\n        \"question\": \"According to the judgment of Apple Court, summarize Zhang's criminal acts.\",\n        \"ref\": [\n            \"Zhang stole a phone worth 2,000 yuan on November 1, 2020.\",\n            \"Zhang stole a wallet worth 40,000 yuan on December 5, 2018.\",\n            \"Zhang stole a watch worth 3,000 yuan in January 2023.\"\n        ],\n        \"answer\": \"Zhang stole a phone worth 2,000 yuan on November 1, 2020, a wallet worth 40,000 yuan on December 5, 2018, and a watch worth 3,000 yuan in January 2023.\"\n    }},\n    {{\n        \"question type\": \"Irrelevant Unsolvable Question\",\n        \"question\": \"According to the judgment of Apple Court, summarize the evidence of Zhang's crimes.\",\n        \"ref\": [],\n        \"answer\": \"Unable to answer.\"\n    }}\n]\n\nPlease refer to the structure of the above examples and construct the corresponding references and answers based on the provided questions. Output in JSON array format.\n\nThe article is as follows: {doc}\nThe questions are as follows: {qa_pairs}\n"}
{"prompt_type": "multi document reference", "system_prompt": "You are an expert in matching questions with corresponding reference answers.", "user_prompt": "Please find the reference for the provided questions and answers in the article, and optimize the answers based on the references. Follow these steps:\n\n1. For each provided question and answer, find the source fragments in the article, also known as references, and note the source of the article. If multiple sentences serve as references, organize them as an array. You need to find all the references in the article, not just one or two sentences, recalling all relevant fragments. References can be based on answer key points but should not be limited to the answer. Try to find as many references as possible in the article. Avoid summary sentences as references; if a reference is a summary sentence, ensure that all the information points mentioned in the answer are described in individual sentences in the reference array. References must be verbatim from the article without any deletion or rewriting. If no corresponding reference is found, output an empty string and do not include unrelated sentences.\n2. Optimize the existing answer based on the references. If the reference contains content not in the answer, supplement the answer. If the answer contains content not found in the reference, check the article for any corresponding references. If there are missing references, add them to the reference array and keep the answer unchanged. If no corresponding reference is found in the article, remove the unrelated content from the answer. Do not stack the answer based on the reference. The answer should be logically clear, fluent, concise, and straightforward. If the current references do not provide the answer to the question, output \"Unable to answer.\" in the answer field.\n\nBelow is a set of sample questions, references, and answers to guide you on how to format the output:\n\nSample question types, questions, and answers:\n[\n    {{\n        \"question type\": \"Multi-document Information Integration Question\",\n        \"common point\": \"Defendant's occupation\",\n        \"question\": \"According to the judgment of Apple Court and Pear Court, what are the occupations of the defendants Zhang and Li?\",\n        \"ref\": [\n            {{\n                \"court_name\": \"Apple Court\",\n                \"content\": \"Defendant: Zhang, male, doctor\"\n            }},\n            {{\n                \"court_name\": \"Pear Court\",\n                \"content\": \"Defendant: Li, male, lawyer\"\n            }}\n        ],\n        \"answer\": \"According to the judgment of Apple Court, the defendant Zhang is a doctor; according to the judgment of Pear Court, the defendant Li is a lawyer.\"\n    }},\n    {{\n        \"question type\": \"Multi-document Comparison Question\",\n        \"common point\": \"Sentencing time\",\n        \"question\": \"According to the judgment of Apple Court and Pear Court, whose sentencing time is longer, Zhang or Li?\",\n        \"ref\": [\n            {{\n                \"court_name\": \"Apple Court\",\n                \"content\": \"Zhang was sentenced to three years of imprisonment\"\n            }},\n            {{\n                \"court_name\": \"Pear Court\",\n                \"content\": \"Li was sentenced to life imprisonment\"\n            }}\n        ],\n        \"answer\": \"Zhang was sentenced to three years of imprisonment, while Li was sentenced to life imprisonment. Li's sentencing time is longer.\"\n    }},\n    {{\n        \"question type\": \"Multi-document Time Sequence Question\",\n        \"common point\": \"Judgment time\",\n        \"question\": \"According to the judgment of Apple Court and Pear Court, whose judgment time is earlier, Zhang or Li?\",\n        \"ref\": [\n            {{\n                \"court_name\": \"Apple Court\",\n                \"content\": \"Judgment time: December 2, 2020\"\n            }},\n            {{\n                \"court_name\": \"Pear Court\",\n                \"content\": \"Judgment time: February 12, 2018\"\n            }}\n        ],\n        \"answer\": \"Li's judgment time is earlier.\"\n    }}\n]\n \nArticle 1 is as follows: {doc_1},\nArticle 2 is as follows: {doc_2},\nQuestions are as follows: {qa_pairs}\n\nPlease strictly follow the structure of the above examples and construct the corresponding references and answers based on the provided question types and questions. Output in JSON format. You need to answer all the questions in the list, not just the first one. Each question needs a corresponding reference, not just an answer.\n"}
{"prompt_type": "Irrelevant Unsolvable Question", "system_prompt": "You are an expert in generating irrelevant and unanswerable questions.", "user_prompt": "Irrelevant and unsolvable questions are questions that cannot be answered based on the content of the article. Please generate irrelevant questions according to the article provided. Irrelevant questions should not be answered, but your court name should match the court name in the article to make sure the question is clear and accurate. Please start each question with \"According to the judgment of the xx court\".Please generate irrelevant questions based on the court name in the article, do not make up the court name or output irrelevant content. Below is the format for constructing irrelevant and unanswerable questions:\n\n[\n    {{\n        \"court_name\": \"Apple Court\",\n        \"question type\": \"Irrelevant Unsolvable Question\",\n        \"question\": \"According to the judgment of Apple Court, what is the profession of the defendant Zhang?\",\n        \"ref\": [\n            \"\"\n        ],\n        \"answer\": \"Unable to answer.\"\n    }}\n]\n\nThe article is as follows: {doc}\nPlease strictly follow the structure of the above example to construct corresponding irrelevant and unanswerable questions based on the article. Output in JSON format.\n"}
